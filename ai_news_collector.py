import requests
import feedparser
import json
from datetime import datetime, timedelta
from typing import List, Dict
import time
from dataclasses import dataclass
import anthropic
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import markdown2
from dotenv import load_dotenv
import os
import sys
from jinja2 import Environment, FileSystemLoader

@dataclass
class NewsItem:
    title: str
    url: str
    published: datetime
    content: str
    source: str

class AINewsCollector:
    # ã‚¯ãƒ©ã‚¹å®šæ•°
    DEFAULT_RSS_FEEDS = [
        "https://feeds.feedburner.com/venturebeat/SZYF",  # VentureBeat AI
        "https://www.artificialintelligence-news.com/feed/",  # AI News
        "https://feeds.feedburner.com/oreilly/radar/atom",  # O'Reilly Radar
        "https://blog.openai.com/rss.xml",  # OpenAI Blog
        "https://deepmind.com/blog/feed/basic/",  # DeepMind
    ]
    
    NEWS_API_KEYWORDS = [
        "artificial intelligence", "machine learning", "deep learning",
        "ChatGPT", "OpenAI", "Google AI", "Microsoft AI", "NVIDIA AI"
    ]
    
    AI_FILTER_KEYWORDS = [
        'AI', 'artificial intelligence', 'machine learning', 'deep learning',
        'neural network', 'ChatGPT', 'OpenAI', 'Google AI', 'Microsoft AI',
        'NVIDIA', 'autonomous', 'computer vision', 'natural language processing',
        'LLM', 'large language model', 'generative AI', 'AGI'
    ]
    
    CLAUDE_MODEL = "claude-opus-4-20250514"
    MAX_NEWS_ITEMS_FOR_SUMMARY = 20
    NEWS_API_PAGE_SIZE = 20
    NEWS_API_RATE_LIMIT_DELAY = 1  # seconds
    
    def __init__(self, anthropic_api_key: str, test_mode: bool = False):
        self.test_mode = test_mode
        if not test_mode:
            self.client = anthropic.Anthropic(api_key=anthropic_api_key)
        else:
            self.client = None
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°
        self.rss_feeds = self.DEFAULT_RSS_FEEDS.copy()
        self.news_api_key = None  # NewsAPIã®ã‚­ãƒ¼ã‚’è¨­å®š
        self.github_url = "https://github.com/HayatoFunahashi/ai_news"

    def _parse_date(self, entry) -> datetime:
        """RSS entryã‹ã‚‰æ—¥ä»˜ã‚’è§£æ"""
        if hasattr(entry, 'published_parsed'):
            return datetime(*entry.published_parsed[:6])
        else:
            return datetime.now()

    def _is_within_timeframe(self, pub_date: datetime, hours_back: int) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸæ™‚é–“å†…ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        return pub_date > cutoff_time
        
    def collect_rss_news(self, hours_back: int = 24) -> List[NewsItem]:
        """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        news_items = []
        
        for feed_url in self.rss_feeds:
            try:
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries:
                    pub_date = self._parse_date(entry)
                    
                    if self._is_within_timeframe(pub_date, hours_back):
                        content = entry.get('summary', entry.get('description', ''))
                        
                        news_item = NewsItem(
                            title=entry.title,
                            url=entry.link,
                            published=pub_date,
                            content=content,
                            source=feed.feed.get('title', 'Unknown')
                        )
                        news_items.append(news_item)
                        
            except Exception as e:
                print(f"Error processing RSS feed {feed_url}: {e}")
                
        return news_items
    
    def collect_news_api(self, hours_back: int = 24) -> List[NewsItem]:
        """News APIã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        if not self.news_api_key:
            return []
            
        news_items = []
        
        # AIé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        keywords = self.NEWS_API_KEYWORDS
        
        from_date = (datetime.now() - timedelta(hours=hours_back)).strftime('%Y-%m-%d')
        
        for keyword in keywords:
            try:
                url = f"https://newsapi.org/v2/everything"
                params = {
                    'q': keyword,
                    'from': from_date,
                    'sortBy': 'publishedAt',
                    'language': 'en',
                    'apiKey': self.news_api_key,
                    'pageSize': self.NEWS_API_PAGE_SIZE
                }
                
                response = requests.get(url, params=params)
                data = response.json()
                
                for article in data.get('articles', []):
                    pub_date = datetime.fromisoformat(
                        article['publishedAt'].replace('Z', '+00:00')
                    ).replace(tzinfo=None)
                    
                    news_item = NewsItem(
                        title=article['title'],
                        url=article['url'],
                        published=pub_date,
                        content=article.get('description', ''),
                        source=article['source']['name']
                    )
                    news_items.append(news_item)
                    
                time.sleep(self.NEWS_API_RATE_LIMIT_DELAY)  # APIåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                print(f"Error collecting news for keyword '{keyword}': {e}")
                
        return news_items
    
    def filter_and_deduplicate(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨é‡è¤‡é™¤å»"""
        # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        ai_keywords = self.AI_FILTER_KEYWORDS
        
        filtered_items = []
        seen_titles = set()
        
        for item in news_items:
            # AIé–¢é€£ã‹ãƒã‚§ãƒƒã‚¯
            is_ai_related = any(
                keyword.lower() in item.title.lower() or 
                keyword.lower() in item.content.lower()
                for keyword in ai_keywords
            )
            
            if is_ai_related and item.title not in seen_titles:
                filtered_items.append(item)
                seen_titles.add(item.title)
                
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        return sorted(filtered_items, key=lambda x: x.published, reverse=True)
    
    def load_test_data(self) -> tuple[List[NewsItem], str]:
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open('test_data.json', 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            # ãƒ†ã‚¹ãƒˆç”¨NewsItemsã‚’ä½œæˆ
            test_items = []
            for item_data in test_data['test_news_items']:
                news_item = NewsItem(
                    title=item_data['title'],
                    url=item_data['url'],
                    published=datetime.fromisoformat(item_data['published']),
                    content=item_data['content'],
                    source=item_data['source']
                )
                test_items.append(news_item)
            
            return test_items, test_data['expected_summary']
            
        except Exception as e:
            print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return [], "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
    
    def _create_summary_prompt(self, news_items: List[NewsItem]) -> str:
        """è¦ç´„ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        news_text = "\n\n".join([
            f"ã‚¿ã‚¤ãƒˆãƒ«: {item.title}\n"
            f"ã‚½ãƒ¼ã‚¹: {item.source}\n"
            f"å†…å®¹: {item.content}\n"
            f"URL: {item.url}"
            for item in news_items[:self.MAX_NEWS_ITEMS_FOR_SUMMARY]
        ])
        
        return f"""
"ã‚ãªãŸã¯AIé–¢é€£ã®çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã™ã‚‹ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚"
"æŠ•è³‡å®¶ãŒç´ æ—©ãåˆ¤æ–­ã§ãã‚‹ã‚ˆã†ã€è¤‡æ•°è¨˜äº‹ã®è¦ç‚¹ã‚’ç°¡æ½”ã«ã€ã‹ã¤å‡ºå…¸ã‚’å«ã‚ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n\n"        
ä»¥ä¸‹ã®AIé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦ã€æŠ•è³‡åˆ¤æ–­ã«å½¹ç«‹ã¤è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãã‚Œãã‚Œã®è¦ç´„ã«ã¯ã€å‡ºå…¸ã€‘ã¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã¨URLã‚‚æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

{news_text}

å‡ºåŠ›å½¢å¼ã¯ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ã‚ã‚Šä»¥ä¸‹ã«ã—ãŸãŒã£ã¦ãã ã•ã„ï¼š

---
- ã‚¿ã‚¤ãƒˆãƒ«: â—¯â—¯â—¯
- æ¦‚è¦: ...
- å½±éŸ¿: ...
- å‡ºå…¸: â—¯â—¯ï¼ˆURLï¼‰
---

ä»¥ä¸‹ã®è¦³ç‚¹ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š
1. ä¸»è¦ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚„å‹•å‘
2. æ³¨ç›®ã™ã¹ãä¼æ¥­ã‚„æŠ€è¡“
3. å¸‚å ´ã¸ã®æ½œåœ¨çš„å½±éŸ¿
4. æŠ•è³‡å®¶ãŒæ³¨ç›®ã™ã¹ããƒã‚¤ãƒ³ãƒˆ

ã‚ã‹ã‚Šã‚„ã™ãã€å…·ä½“æ€§ã®ã‚ã‚‹æ—¥æœ¬èªã§ç°¡æ½”ã«æ›¸ã„ã¦ãã ã•ã„ã€‚
"""

    def summarize_with_claude(self, news_items: List[NewsItem]) -> str:
        """Claudeã‚’ä½¿ã£ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦ç´„ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        if self.test_mode:
            # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼šå›ºå®šã®è¦ç´„ã‚’è¿”ã™
            _, test_summary = self.load_test_data()
            print("[TEST MODE] å›ºå®šã®è¦ç´„ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
            return test_summary
        
        if not news_items:
            return "ä»Šæ—¥ã¯AIé–¢é€£ã®é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        prompt = self._create_summary_prompt(news_items)
        
        try:
            message = self.client.messages.create(
                model=self.CLAUDE_MODEL,
                max_tokens=1000,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            return message.content[0].text
            
        except Exception as e:
            print(f"Claude API error: {e}")
            return "è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    
    def _create_html_content(self, summary: str, news_items: List[NewsItem]) -> str:
        """HTMLãƒ¡ãƒ¼ãƒ«ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        env = Environment(loader=FileSystemLoader('templates'))
        template = env.get_template('email_template.html')

        # Markdownè¦ç´„ã‚’HTMLã«å¤‰æ›
        summary_html = markdown2.markdown(summary)

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿
        return template.render(
            date=datetime.now().strftime('%Y-%m-%d'),
            generated_at=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            news_items=news_items[:self.MAX_NEWS_ITEMS_FOR_SUMMARY],
            summary_html=summary_html
        )

    def send_email_summary(self, summary: str, recipient_emails: List[str], 
                          smtp_server: str, smtp_port: int, 
                          sender_email: str, sender_password: str,
                          news_items: List[NewsItem]):
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ã£ã¦è¤‡æ•°ã®å®›å…ˆã«HTMLãƒ¡ãƒ¼ãƒ«ã‚’é€ä¿¡"""
        if not recipient_emails:
            print("é€ä¿¡å…ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
            
        try:
            html_content = self._create_html_content(summary, news_items)

            # SMTPæ¥ç¶šã‚’ä¸€åº¦ã ã‘ç¢ºç«‹
            server = smtplib.SMTP(smtp_server, smtp_port)
            server.starttls()
            server.login(sender_email, sender_password)
            
            success_count = 0
            failed_recipients = []
            
            # å„å—ä¿¡è€…ã«å€‹åˆ¥ã«ãƒ¡ãƒ¼ãƒ«é€ä¿¡
            for recipient_email in recipient_emails:
                try:
                    # ãƒ¡ãƒ¼ãƒ«æ§‹ç¯‰
                    msg = MIMEMultipart()
                    msg['From'] = sender_email
                    msg['To'] = recipient_email
                    msg['Subject'] = f"ğŸ¤– AI News Summary - {datetime.now().strftime('%Y-%m-%d')}"
                    msg.attach(MIMEText(html_content, 'html', 'utf-8'))

                    # å€‹åˆ¥é€ä¿¡
                    server.send_message(msg)
                    success_count += 1
                    print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡æˆåŠŸ: {recipient_email}")
                    
                except Exception as e:
                    failed_recipients.append(recipient_email)
                    print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡å¤±æ•— ({recipient_email}): {e}")
            
            server.quit()
            
            print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œäº†: æˆåŠŸ {success_count}ä»¶, å¤±æ•— {len(failed_recipients)}ä»¶")
            if failed_recipients:
                print(f"é€ä¿¡å¤±æ•—ã—ãŸå®›å…ˆ: {', '.join(failed_recipients)}")
            
        except Exception as e:
            print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼ (SMTPæ¥ç¶š): {e}")

    def _collect_all_news(self) -> List[NewsItem]:
        """å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’åé›†"""
        if self.test_mode:
            print("[TEST MODE] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")
            filtered_news, _ = self.load_test_data()
            print(f"ãƒ†ã‚¹ãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(filtered_news)}")
            return filtered_news
        else:
            # æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ï¼šå®Ÿéš›ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            rss_news = self.collect_rss_news()
            api_news = self.collect_news_api()
            all_news = rss_news + api_news
            
            print(f"åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(all_news)}")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»é‡è¤‡é™¤å»
            filtered_news = self.filter_and_deduplicate(all_news)
            print(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_news)}")
            return filtered_news

    def _save_results(self, filtered_news: List[NewsItem], summary: str) -> str:
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_data = {
            'timestamp': timestamp,
            'summary': summary,
            'news_count': len(filtered_news),
            'news_items': [
                {
                    'title': item.title,
                    'url': item.url,
                    'published': item.published.isoformat(),
                    'source': item.source
                }
                for item in filtered_news
            ]
        }
        
        with open(f'ai_news_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜
        with open(f'ai_news_summary_{timestamp}.txt', 'w', encoding='utf-8') as f:
            f.write(summary)
        
        print("ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")
        return timestamp

    def _get_email_settings(self) -> Dict[str, any]:
        """ãƒ¡ãƒ¼ãƒ«è¨­å®šã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—"""
        return {
            'smtp_server': os.getenv('SMTP_SERVER'),
            'smtp_port': int(os.getenv('SMTP_PORT', '587')),
            'sender_email': os.getenv('EMAIL_ADDRESS'),
            'sender_password': os.getenv('EMAIL_PASSWORD')
        }

    def run_daily_collection(self, recipient_emails: List[str] = None):
        """æ—¥æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»è¦ç´„ãƒ»é…ä¿¡"""
        print(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹: {datetime.now()}")
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
        filtered_news = self._collect_all_news()
             
        # Claude ã§è¦ç´„
        summary = self.summarize_with_claude(filtered_news)
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self._save_results(filtered_news, summary)
        
        # ãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        if recipient_emails and len(recipient_emails) > 0:
            email_settings = self._get_email_settings()
            self.send_email_summary(
                summary, 
                recipient_emails,
                **email_settings,
                news_items=filtered_news
            )
        
        return summary

    def generate_dashboard_data(self):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        print("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        
        # æ—¢å­˜ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        json_files = []
        for file in os.listdir('.'):
            if file.startswith('ai_news_') and file.endswith('.json'):
                json_files.append(file)
        
        if not json_files:
            print("JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
        all_news_items = []
        summaries = []
        
        for json_file in sorted(json_files):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚ˆã‚Šä½¿ã„ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
                if 'timestamp' in data:
                    # YYYYMMDD_HHMMSS -> ISO format
                    timestamp_str = data['timestamp']
                    try:
                        timestamp_dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                        iso_timestamp = timestamp_dt.isoformat()
                    except ValueError:
                        iso_timestamp = timestamp_str
                else:
                    iso_timestamp = datetime.now().isoformat()
                
                # ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’è¿½åŠ 
                if 'news_items' in data:
                    for item in data['news_items']:
                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
                        if not any(existing['url'] == item['url'] for existing in all_news_items):
                            all_news_items.append(item)
                
                # ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
                summary_item = {
                    'timestamp': iso_timestamp,
                    'summary': data.get('summary', ''),
                    'news_count': data.get('news_count', 0),
                    'headlines': data.get('headlines', '')
                }
                summaries.append(summary_item)
                
            except Exception as e:
                print(f"ãƒ•ã‚¡ã‚¤ãƒ« {json_file} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        # é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        aggregated_data = {
            'generated_at': datetime.now().isoformat(),
            'total_summaries': len(summaries),
            'total_news_items': len(all_news_items),
            'news_items': sorted(all_news_items, key=lambda x: x['published'], reverse=True),
            'summaries': sorted(summaries, key=lambda x: x['timestamp'], reverse=True)
        }
        
        # docsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        os.makedirs('docs/data', exist_ok=True)
        
        # é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        output_file = 'docs/data/aggregated_news.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(aggregated_data, f, ensure_ascii=False, indent=2)
        
        print(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")
        print(f"- ã‚µãƒãƒªãƒ¼æ•°: {len(summaries)}")
        print(f"- ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®æ•°: {len(all_news_items)}")
        
        return output_file


def parse_recipient_emails(email_env_var: str) -> List[str]:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¤‡æ•°ã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šå¯¾å¿œï¼‰"""
    if not email_env_var:
        return []
    
    # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’åˆ†å‰²ã—ã€ç©ºç™½ã‚’é™¤å»
    emails = [email.strip() for email in email_env_var.split(',')]
    
    # ç©ºã®è¦ç´ ã‚’é™¤å»ã—ã€æœ‰åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®ã¿ã‚’è¿”ã™
    valid_emails = []
    for email in emails:
        if email and '@' in email:  # ç°¡å˜ãªæ¤œè¨¼
            valid_emails.append(email)
        elif email:  # ç„¡åŠ¹ãªã‚¢ãƒ‰ãƒ¬ã‚¹ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
            print(f"è­¦å‘Š: ç„¡åŠ¹ãªãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å½¢å¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: '{email}'")
    
    return valid_emails

# ä½¿ç”¨ä¾‹
def main():
    load_dotenv()  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
    
    # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®šï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ï¼‰
    test_mode = os.getenv("TEST_MODE", "false").lower() == "true" or "--test" in sys.argv
    
    if test_mode:
        print("=== ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­ ===")
        collector = AINewsCollector("", test_mode=True)
    else:
        # API Keyè¨­å®š
        ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
        if not ANTHROPIC_API_KEY:
            print("ã‚¨ãƒ©ãƒ¼: ANTHROPIC_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        collector = AINewsCollector(ANTHROPIC_API_KEY, test_mode=False)
    
    # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯ç„¡åŠ¹ï¼‰
    recipient_emails = None
    if not test_mode:
        # RECIPIENT_EMAILS (è¤‡æ•°å¯¾å¿œ) ã¾ãŸã¯ RECIPIENT_EMAIL (å¾Œæ–¹äº’æ›æ€§) ã‹ã‚‰å–å¾—
        email_env = os.getenv('RECIPIENT_EMAILS') or os.getenv('RECIPIENT_EMAIL')
        recipient_emails = parse_recipient_emails(email_env)
        if recipient_emails:
            print(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡å¯¾è±¡: {len(recipient_emails)}ä»¶ - {', '.join(recipient_emails)}")
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»è¦ç´„å®Ÿè¡Œ
    summary = collector.run_daily_collection(recipient_emails)
    
    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    collector.generate_dashboard_data()
    
    print("\n=== ä»Šæ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ ===")
    print(summary)


if __name__ == "__main__":
    main()
